# 引用
https://openhydra.net/te/

# what is transformers
从根本上讲，文本生成的Transformer模型是基于`next-word prediction`的原理运行的：给定用户输入的文本提示，接下来最有可能跟随这个输入的“下一个单词”是什么呢？Transformer的核心创新和强大之处在于其对自注意力机制的运用，这使得它们能够比以前的架构更有效地处理整个序列，并捕捉长距离的依赖关系。

# Embedding
用户输入需要被转换为模型能够理解和处理的格式。这就是嵌入层的作用：它将文本转换为模型可以处理的数值表示。为了将提示转换为嵌入，我们需要1）对输入进行Tokenization，2）获取标记嵌入，3）添加位置信息，最后4）将标记和位置编码相加，以获得最终的嵌入
## Tokenization
Tokenization是将输入文本分解为称为标记的更小、更易于管理的部分的过程。这些标记可以是一个单词或一个子词。单词 "数据"（"Data"） 和 "可视化"（"visualization"） 对应于唯一的标记，而单词 "使能够"（"empowers"） 被拆分为两个标记。标记的完整词汇表在训练模型之前就已确定：GPT-2的词汇表有 50,257 个唯一标记。现在我们将输入文本拆分为具有不同ID的标记，我们可以从嵌入中获得它们的向量表示。
## Token Embedding
GPT-2（小型）将词汇表中的每个标记表示为一个768维的向量；向量的维度取决于模型。这些嵌入向量存储在一个形状为 (50,257, 768) 的矩阵中
## Positional Encoding
嵌入层还对输入提示中每个标记的位置信息进行编码。不同的模型使用不同的方法进行位置编码。GPT-2从零开始训练自己的位置编码矩阵，并将其直接集成到训练过程中。
## Final Embedding
最后，我们将标记编码和位置编码相加，以获得最终的嵌入表示。这个组合表示既捕获了标记的语义含义，又捕获了它们在输入序列中的位置

# TODO